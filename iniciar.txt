

python -m fl_main.pseudodb.pseudo_db
python -m fl_main.aggregator.server_th

python -m fl_main.examples.image_classification.classification_engine 1 50002 a2
python -m fl_main.examples.image_classification.classification_engine 1 50003 a3
python -m fl_main.examples.image_classification.classification_engine 1 50004 a4

####TESTEO
python -m fl_main.agent.client.py 1 9001 agent1


#DOCKER

sudo docker start -i container2
docker stop container2
docker rm container2

sudo docker ps -q | xargs -r sudo docker stop
sudo docker ps -aq | xargs -r sudo docker rm


sudo docker run -it \
  --name container3 \
  --network flnet \
  -v "$(pwd)":/app:ro \
  -v setups_container2:/app/setups \
  fl-node:py38 \
  bash




###QUE SE ESTA HACIENDO

*El server verifica si db tiene roun >0 y si es el caso usa ell nuevo agregador es aronda
*El server verifica si db tiene round = 0 entonces inicializa con round 0

*Ahora el problema es que si el server_th se muere olvida las ips - sockets
*Solucionar agregar las ips y sockets  en la db
*Luego cada server_th leer las ips y sockets y esperarlas para seguir 





/////
Resumen de la arquitectura deseada
PC (tu ordenador): correrá pseudo_db (base de datos SQLite) y será el punto central de persistencia.
3 Raspberry Pi: ejecutarán la lógica de agente + clasificación; cualquiera puede ser promoted a agregador en tiempo de ejecución.
Rotación: cuando el agregador elige un nuevo aggregator, enviará un mensaje de rotación. Los agentes actualizan config_agent.json y config_aggregator.json; el supervisor (en cada Pi) detectará el cambio y arrancará server_th automáticamente.
Requisitos previos en cada máquina
Python 3.8+ con dependencias del repo instaladas (usa virtualenv si prefieres).
Código del repo disponible en cada dispositivo (git clone o rsync).
En el PC (DB host) abrir puerto db_socket (por defecto 9017) si usas red local y quieres que los agregadores contacten la pseudo-db vía websockets. También deben abrirse puertos de agregador cuando actúe como servicio (reg_socket/exch_socket/recv_socket, por defecto 8765/7890/4321).
Asegúrate que los archivos en setups/ son editables por los procesos (no estarán montados como read-only).
Ajustes específicos en setups (recomendación)
En el PC (DB host): setups/config_db.json:
db_ip: la IP LAN del PC (ej. "192.168.1.10")
db_socket: "9017" (o puerto que el pseudo-db exponga)
En la Raspberry Pi que starts as initial aggregator:
config_aggregator.json:
aggr_ip: su IP local (ej. "192.168.1.101")
reg_socket: "8765" (o el puerto elegido)
recv_socket, exch_socket: mantener coherentes (ej. 4321/7890)
db_ip: IP del PC (ej. "192.168.1.10")
role: "aggregator"
En las otras Raspberry Pi (agents):
config_agent.json:
aggr_ip: IP del initial aggregator (ej. "192.168.1.101")
reg_socket: "8765"
polling: 1 (si usas polling mode)
role: "agent"
config_aggregator.json on these agents should reference the current aggregator (initially the same IP as above) and role: "agent"; the rotation broadcast will update these when needed.
En los 3 Pis y en el PC: setups/config_db.json must point to the PC's IP so aggregator can push to DB.
Puertos a abrir / firewall
En el PC (DB):
9017/tcp — pseudo_db websocket port
En cada Pi (as potential aggregator):
8765/tcp — registration websocket
4321/tcp — recv/polling socket
7890/tcp — exchange socket (if used)
Asegura conectividad desde cada Pi al PC IP:9017 and between Pis on reg_socket ports.
Cómo iniciar todo (orden recomendado)
En PC (DB):
En Pi 1 (initial aggregator):
Copia setups/ y deja config_aggregator.json con role: "aggregator" y aggr_ip = Pi1 IP.
Espera que el agregador muestre en logs: "Aggregator Started".
En Pi 2 y Pi 3:
Copia repo, ajusta config_agent.json apuntando a Pi1 IP and reg_socket 8765; role: "agent".
Inicia el supervisor que lanzará la clasificación:
Observa logs en agregador y en Pis. En modo simulación los agentes enviarán un modelo inicial y el agregador podrá agregar y luego ejecutar rotación.
Autonomía y rotación — cómo ocurre sin intervención
Cuando el agregador completa una ronda, ejecuta _choose_and_broadcast_new_aggregator():
Genera rotation message y lo envía a cada agente (incluyendo al ganador).
Cada agente maneja el message en wait_models() y actualiza su config_agent.json y config_aggregator.json (cambia role localmente).
En cada Raspberry Pi el supervisor role_supervisor_classification.py detectará role == 'aggregator' y hará exec de server_th. Eso transforma la Pi en agregador sin intervención manual.
El antiguo agregador actualizará su configs para role: agent y su supervisor (o proceso) volverá a comportarse como agente.
Recomendaciones adicionales / robustecimientos
Asegura que los archivos setups no estén montados como read-only en las Pis — los agentes necesitan poder escribir config_agent.json para promoverse.
Considera usar TLS/auth for websockets if exposing across networks.
Para producción: reemplazar pickle+websockets with authenticated transport; add retries + confirmation for rotation messages (guarantee delivery), and add health checks and systemd units on Pis for resilience.
Opcional: usar rsync or a deployment script to propagate setups/ to all Pis and ensure correct permissions.
¿Quieres que implemente estos puntos en el repo?
Puedo:
A) añadir el role_supervisor_classification.py (ya lo hice),
B) aplicar un patch adicional para que classification_engine revise role at startup (no estrictamente necesario thanks to the supervisor),
C) add small docs/README with the per-device commands and exact config templates for the 4 machines,
D) add systemd unit samples for each role.
Dime si quieres que:
genere los config_*.json ejemplos listos para copiar (con plantillas y variables como IPs), y/o
aplique parches adicionales (p.ej. force-cast ports to int in server_th.py), y/o
genere un script de despliegue (deploy_to_pi.sh) que haga rsync + set permissions + start supervisor on each Pi (requiere SSH keys).
Si quieres, puedo ahora:

crear los ejemplos de config_agent.json y config_aggregator.json preconfigurados para tus 3 Pis y tu PC (pediré las IPs), y un pequeño README_deploy.md con comandos exactos; o
aplicar el port-casting patch en server_th.py para reducir riesgos de que websockets no escuche por pasar strings.
Indica qué prefieres (plantillas + README, o patches automáticos + reinicio, o ambos) y lo hago